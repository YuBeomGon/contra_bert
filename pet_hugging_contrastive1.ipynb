{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow_datasets\n",
    "from transformers import *\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tqdm.notebook import tqdm\n",
    "# from wandb.keras import WandbCallback\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# !wget https://raw.githubusercontent.com/wangz10/contrastive_loss/master/losses.py\n",
    "import losses\n",
    "import feed\n",
    "from feed import *\n",
    "import time\n",
    "import model\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('files/train.csv')\n",
    "val_df = pd.read_csv('files/val.csv')\n",
    "test_df = pd.read_csv('files/test.csv')\n",
    "pred_df = pd.read_csv('files/pred.csv')\n",
    "Num_class = len(set(train_df.label.value_counts()))\n",
    "print(Num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 128\n",
    "REPEAT = 2\n",
    "NUM_TRAIN_ITERATION = REPEAT * int(len(train_df) / BATCH_SIZE)\n",
    "NUM_TEST_ITERATION = int(len(test_df) / BATCH_SIZE)\n",
    "print(NUM_TRAIN_ITERATION)\n",
    "print(NUM_TEST_ITERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AlbertTokenizer, TFAlbertForSequenceClassification\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "# model = TFAlbertModel.from_pretrained('albert-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/wangz10/contrastive_loss/blob/master/model.py\n",
    "class UnitNormLayer(tf.keras.layers.Layer):\n",
    "    '''Normalize vectors (euclidean norm) in batch to unit hypersphere.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(UnitNormLayer, self).__init__()\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        norm = tf.norm(input_tensor, axis=1)\n",
    "        return input_tensor / tf.reshape(norm, [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Network\n",
    "# max_seq_length = 128\n",
    "# BATCH_SIZE = 32\n",
    "def encoder_net():\n",
    "#     input_ids = keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "#     attention_mask = keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "#     token_type_ids = keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    input_ids = tf.keras.Input(shape=(MAX_SEQ_LEN,), batch_size=BATCH_SIZE, name=\"input_ids\", dtype=tf.int32)\n",
    "    attention_mask = tf.keras.Input(shape=(MAX_SEQ_LEN,), batch_size=BATCH_SIZE, name=\"attention_mask\", dtype=tf.int32)\n",
    "    token_type_ids = tf.keras.Input(shape=(MAX_SEQ_LEN,), batch_size=BATCH_SIZE, name=\"token_type_ids\", dtype=tf.int32)\n",
    "\n",
    "    bert_input = [input_ids, attention_mask, token_type_ids]\n",
    "#     normalization_layer = UnitNormLayer()\n",
    "    \n",
    "#     input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\"))[None, :]\n",
    "\n",
    "    encoder = TFAlbertModel.from_pretrained('albert-base-v2')\n",
    "    encoder.trainable = True\n",
    "\n",
    "    embeddings = encoder(bert_input)\n",
    "#     norm_embeddings = normalization_layer(embeddings)\n",
    "    encoder_network = tf.keras.Model(inputs=bert_input, outputs=embeddings)\n",
    "\n",
    "    return encoder_network\n",
    "\n",
    "# Projector Network\n",
    "def projector_net():\n",
    "    projector = tf.keras.models.Sequential([\n",
    "        Dense(128, activation=\"relu\"),\n",
    "#         UnitNormLayer()\n",
    "    ])\n",
    "\n",
    "    return projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to InputExample format\n",
    "\n",
    "train_examples = convert_text_to_examples(train_df['text'], train_df['label_id'])\n",
    "val_examples = convert_text_to_examples(val_df['text'], val_df['label_id'])\n",
    "test_examples = convert_text_to_examples(test_df['text'], test_df['label_id'])\n",
    "pred_examples = convert_text_to_examples(pred_df['text'], np.zeros(len(pred_df) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to features\n",
    "# max_seq_length = 128\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels ) = convert_examples_to_features(tokenizer, train_examples, max_seq_length=MAX_SEQ_LEN)\n",
    "(val_input_ids, val_input_masks, val_segment_ids, val_labels ) = convert_examples_to_features(tokenizer, val_examples, max_seq_length=MAX_SEQ_LEN)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels ) = convert_examples_to_features(tokenizer, test_examples, max_seq_length=MAX_SEQ_LEN)\n",
    "(pred_input_ids, pred_input_masks, pred_segment_ids, pred_labels ) = convert_examples_to_features(tokenizer, pred_examples, max_seq_length=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE=32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": tf.constant( train_input_ids, dtype=tf.int32),\n",
    "                                                  \"attention_mask\": tf.constant( train_input_masks, dtype=tf.int32),\n",
    "                                                  \"token_type_ids\": tf.constant( train_segment_ids, dtype=tf.int32),}\n",
    "                                                    ,tf.constant(train_labels))).repeat()\n",
    "train_dataset = train_dataset.shuffle(100, reshuffle_each_iteration=True).batch(BATCH_SIZE,drop_remainder=True).repeat(2)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": tf.constant( val_input_ids, dtype=tf.int32),\n",
    "                                                  \"attention_mask\": tf.constant( val_input_masks, dtype=tf.int32),\n",
    "                                                  \"token_type_ids\": tf.constant( val_segment_ids, dtype=tf.int32),}\n",
    "                                                    ,tf.constant(val_labels)))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE,drop_remainder=False)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": tf.constant( test_input_ids, dtype=tf.int32),\n",
    "                                                  \"attention_mask\": tf.constant( test_input_masks, dtype=tf.int32),\n",
    "                                                  \"token_type_ids\": tf.constant( test_segment_ids, dtype=tf.int32),}\n",
    "                                                    ,tf.constant(test_labels)))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE,drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "encoder_r = encoder_net()\n",
    "projector_z = projector_net()\n",
    "normalization_layer = UnitNormLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(bert_input, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        r = encoder_r(bert_input, training=True)\n",
    "        r = r[0][:,0,:] # take cls token\n",
    "#         r = tf.keras.layers.GlobalAveragePooling1D()(r[0])\n",
    "        r = normalization_layer(r)\n",
    "        z = projector_z(r, training=True)\n",
    "        z = normalization_layer(z)\n",
    "        loss = losses.max_margin_contrastive_loss(z, labels, metric='cosine')\n",
    "\n",
    "    gradients = tape.gradient(loss, \n",
    "        encoder_r.trainable_variables + projector_z.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, \n",
    "        encoder_r.trainable_variables + projector_z.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sleep 3h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3519efeb5b4068989f28f836cc14b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/beomgon2/huggingface/hugging-venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-14-6d5630efa0f1>\", line 13, in <module>\n",
      "    loss = train_step(bert_input, labels)\n",
      "  File \"/home/beomgon2/huggingface/hugging-venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 580, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/beomgon2/huggingface/hugging-venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\", line 611, in _call\n",
      "    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/beomgon2/huggingface/hugging-venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 2420, in __call__\n",
      "    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n",
      "  File \"/home/beomgon2/huggingface/hugging-venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1665, in _filtered_call\n",
      "    self.captured_inputs)\n",
      "  File \"/home/beomgon2/huggingface/hugging-venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 1746, in _call_flat\n",
      "    ctx, args, cancellation_manager=cancellation_manager))\n",
      "  File \"/home/beomgon2/huggingface/hugging-venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py\", line 598, in call\n",
      "    ctx=ctx)\n",
      "  File \"/home/beomgon2/huggingface/hugging-venv/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n",
      "    inputs, attrs, num_outputs)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/beomgon2/huggingface/hugging-venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/beomgon2/huggingface/hugging-venv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/beomgon2/huggingface/hugging-venv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/beomgon2/huggingface/hugging-venv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 362, in _fixed_getinnerframes\n",
      "    aux = traceback.extract_tb(etb)\n",
      "  File \"/usr/lib/python3.6/traceback.py\", line 72, in extract_tb\n",
      "    return StackSummary.extract(walk_tb(tb), limit=limit)\n",
      "  File \"/usr/lib/python3.6/traceback.py\", line 345, in extract\n",
      "    for f, lineno in frame_gen:\n",
      "  File \"/usr/lib/python3.6/traceback.py\", line 311, in walk_tb\n",
      "    tb = tb.tb_next\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "# wandb.init(project=\"scl\", entity=\"authors\", id=\"supervised-contrastive-crctd\")\n",
    "EPOCHS = 5\n",
    "LOG_EVERY = 1\n",
    "train_loss_results = []\n",
    "\n",
    "test_count = 0\n",
    "\n",
    "start = time.time()\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "    for (bert_input, labels) in train_dataset:\n",
    "        loss = train_step(bert_input, labels)\n",
    "        epoch_loss_avg.update_state(loss) \n",
    "\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "#     wandb.log({\"supervised_contrastive_loss\": epoch_loss_avg.result()})\n",
    "\n",
    "    if epoch % LOG_EVERY == 0:\n",
    "        print(\"Epoch: {} Loss: {:.3f}\".format(epoch, epoch_loss_avg.result()))\n",
    "\n",
    "end = time.time()\n",
    "print('training duration is ', end - start)\n",
    "# wandb.log({\"training_time\": end - start})\n",
    "\n",
    "with plt.xkcd():\n",
    "    plt.plot(train_loss_results)\n",
    "    plt.title(\"Supervised Contrastive Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_model():\n",
    "    input_ids = tf.keras.Input(shape=(MAX_SEQ_LEN,), batch_size=BATCH_SIZE, name=\"input_ids\", dtype=tf.int32)\n",
    "    attention_mask = tf.keras.Input(shape=(MAX_SEQ_LEN,), batch_size=BATCH_SIZE, name=\"attention_mask\", dtype=tf.int32)\n",
    "    token_type_ids = tf.keras.Input(shape=(MAX_SEQ_LEN,), batch_size=BATCH_SIZE, name=\"token_type_ids\", dtype=tf.int32)\n",
    "\n",
    "    bert_input = [input_ids, attention_mask, token_type_ids]\n",
    "    encoder_r.trainable = False\n",
    "    r = encoder_r(bert_input, training=False)\n",
    "\n",
    "#     encoder_r.trainable = True\n",
    "#     r = encoder_r(bert_input, training=True)\n",
    "    r = r[0][:,0,:]\n",
    "    r = normalization_layer(r)\n",
    "#     r = tf.keras.layers.GlobalAveragePooling1D()(r[0])\n",
    "    outputs = Dense(Num_class)(r)\n",
    "\n",
    "    supervised_model = tf.keras.Model(bert_input, outputs)\n",
    "\n",
    "    return supervised_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_classifier = supervised_model()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "supervised_classifier.compile(optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metric)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2,\n",
    "    restore_best_weights=True, verbose=2)\n",
    "\n",
    "supervised_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_classifier.fit(train_dataset,steps_per_epoch= NUM_TRAIN_ITERATION,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=2,\n",
    "    callbacks= es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_classifier.evaluate(test_dataset, steps = NUM_TEST_ITERATION )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
