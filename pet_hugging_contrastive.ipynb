{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import tensorflow_datasets\n",
    "from transformers import *\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tqdm.notebook import tqdm\n",
    "# from wandb.keras import WandbCallback\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# !wget https://raw.githubusercontent.com/wangz10/contrastive_loss/master/losses.py\n",
    "import losses\n",
    "import feed\n",
    "from feed import *\n",
    "import time\n",
    "import model\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('files/train.csv')\n",
    "val_df = pd.read_csv('files/val.csv')\n",
    "test_df = pd.read_csv('files/test.csv')\n",
    "pred_df = pd.read_csv('files/pred.csv')\n",
    "Num_class = len(set(train_df.label.value_counts()))\n",
    "print(Num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "MAX_SEQ_LEN = 128\n",
    "REPEAT = 2\n",
    "NUM_TRAIN_ITERATION = REPEAT * int(len(train_df) / BATCH_SIZE)\n",
    "NUM_TEST_ITERATION = int(len(test_df) / BATCH_SIZE)\n",
    "print(NUM_TRAIN_ITERATION)\n",
    "print(NUM_TEST_ITERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AlbertTokenizer, TFAlbertForSequenceClassification\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "# model = TFAlbertModel.from_pretrained('albert-base-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/wangz10/contrastive_loss/blob/master/model.py\n",
    "class UnitNormLayer(tf.keras.layers.Layer):\n",
    "    '''Normalize vectors (euclidean norm) in batch to unit hypersphere.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(UnitNormLayer, self).__init__()\n",
    "\n",
    "    def call(self, input_tensor):\n",
    "        norm = tf.norm(input_tensor, axis=1)\n",
    "        return input_tensor / tf.reshape(norm, [-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Network\n",
    "# max_seq_length = MAX_SEQ_LEN\n",
    "# BATCH_SIZE = 32\n",
    "def encoder_net():\n",
    "#     input_ids = keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "#     attention_mask = keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "#     token_type_ids = keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    input_ids = tf.keras.Input(shape=(MAX_SEQ_LEN,), batch_size=BATCH_SIZE, name=\"input_ids\", dtype=tf.int32)\n",
    "    attention_mask = tf.keras.Input(shape=(MAX_SEQ_LEN,), batch_size=BATCH_SIZE, name=\"attention_mask\", dtype=tf.int32)\n",
    "    token_type_ids = tf.keras.Input(shape=(MAX_SEQ_LEN,), batch_size=BATCH_SIZE, name=\"token_type_ids\", dtype=tf.int32)\n",
    "\n",
    "    bert_input = [input_ids, attention_mask, token_type_ids]\n",
    "    \n",
    "#     input_ids = tf.constant(tokenizer.encode(\"Hello, my dog is cute\"))[None, :]\n",
    "\n",
    "    encoder = TFAlbertModel.from_pretrained('albert-base-v2')\n",
    "    encoder.trainable = True\n",
    "\n",
    "    embeddings = encoder(bert_input)\n",
    "    encoder_network = tf.keras.Model(inputs=bert_input, outputs=embeddings)\n",
    "\n",
    "    return encoder_network\n",
    "\n",
    "# Projector Network\n",
    "def projector_net():\n",
    "    projector = tf.keras.models.Sequential([\n",
    "        Dense(128, activation=\"relu\"),\n",
    "#         UnitNormLayer()\n",
    "    ])\n",
    "\n",
    "    return projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_r = encoder_net()\n",
    "# projector_z = projector_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to InputExample format\n",
    "\n",
    "train_examples = convert_text_to_examples(train_df['text'], train_df['label_id'])\n",
    "val_examples = convert_text_to_examples(val_df['text'], val_df['label_id'])\n",
    "test_examples = convert_text_to_examples(test_df['text'], test_df['label_id'])\n",
    "pred_examples = convert_text_to_examples(pred_df['text'], np.zeros(len(pred_df) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to features\n",
    "# max_seq_length = 128\n",
    "(train_input_ids, train_input_masks, train_segment_ids, train_labels ) = convert_examples_to_features(tokenizer, train_examples, max_seq_length=MAX_SEQ_LEN)\n",
    "(val_input_ids, val_input_masks, val_segment_ids, val_labels ) = convert_examples_to_features(tokenizer, val_examples, max_seq_length=MAX_SEQ_LEN)\n",
    "(test_input_ids, test_input_masks, test_segment_ids, test_labels ) = convert_examples_to_features(tokenizer, test_examples, max_seq_length=MAX_SEQ_LEN)\n",
    "(pred_input_ids, pred_input_masks, pred_segment_ids, pred_labels ) = convert_examples_to_features(tokenizer, pred_examples, max_seq_length=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE=32\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": tf.constant( train_input_ids, dtype=tf.int32),\n",
    "                                                  \"attention_mask\": tf.constant( train_input_masks, dtype=tf.int32),\n",
    "                                                  \"token_type_ids\": tf.constant( train_segment_ids, dtype=tf.int32),}\n",
    "                                                    ,tf.constant(train_labels))).repeat()\n",
    "train_dataset = train_dataset.shuffle(100, reshuffle_each_iteration=True).batch(BATCH_SIZE,drop_remainder=True).repeat(2)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": tf.constant( val_input_ids, dtype=tf.int32),\n",
    "                                                  \"attention_mask\": tf.constant( val_input_masks, dtype=tf.int32),\n",
    "                                                  \"token_type_ids\": tf.constant( val_segment_ids, dtype=tf.int32),}\n",
    "                                                    ,tf.constant(val_labels)))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": tf.constant( test_input_ids, dtype=tf.int32),\n",
    "                                                  \"attention_mask\": tf.constant( test_input_masks, dtype=tf.int32),\n",
    "                                                  \"token_type_ids\": tf.constant( test_segment_ids, dtype=tf.int32),}\n",
    "                                                    ,tf.constant(test_labels)))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128)\n"
     ]
    }
   ],
   "source": [
    "al_inputs,  label = next(iter(train_dataset))\n",
    "print(al_inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(al_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(label)\n",
    "# print(al_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing TFAlbertModel: ['predictions']\n",
      "- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the model checkpoint at albert-base-v2.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "encoder_r = encoder_net()\n",
    "projector_z = projector_net()\n",
    "normalization_layer = UnitNormLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = encoder_r(al_inputs)\n",
    "# # print(r)\n",
    "# r1 = r[0][:,0,:]\n",
    "# print(r1.shape)\n",
    "# z = projector_z(r1)\n",
    "# print(tf.shape(r1))\n",
    "# print(tf.shape(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def train_step(bert_input, labels):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         r = encoder_r(bert_input, training=True)\n",
    "#         r = r[0][:,0,:]\n",
    "#         z = projector_z(r, training=True)\n",
    "#         loss = losses.max_margin_contrastive_loss(z, labels, metric='cosine')\n",
    "\n",
    "#     gradients = tape.gradient(loss, \n",
    "#         encoder_r.trainable_variables + projector_z.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, \n",
    "#         encoder_r.trainable_variables + projector_z.trainable_variables))\n",
    "\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # wandb.init(project=\"scl\", entity=\"authors\", id=\"supervised-contrastive-crctd\")\n",
    "# EPOCHS = 5\n",
    "# LOG_EVERY = 10\n",
    "# train_loss_results = []\n",
    "\n",
    "# test_count = 0\n",
    "\n",
    "# start = time.time()\n",
    "# for epoch in tqdm(range(EPOCHS)):\n",
    "#     epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "\n",
    "#     for (bert_input, labels) in train_dataset:\n",
    "# #         if test_count < 2 :\n",
    "# #             print(bert_input)\n",
    "# #             print(labels)\n",
    "# #             test_count += 1\n",
    "#         loss = train_step(bert_input, labels)\n",
    "#         epoch_loss_avg.update_state(loss) \n",
    "\n",
    "#     train_loss_results.append(epoch_loss_avg.result())\n",
    "# #     wandb.log({\"supervised_contrastive_loss\": epoch_loss_avg.result()})\n",
    "\n",
    "#     if epoch % LOG_EVERY == 2:\n",
    "#         print(\"Epoch: {} Loss: {:.3f}\".format(epoch, epoch_loss_avg.result()))\n",
    "\n",
    "# end = time.time()\n",
    "# print('training duration is ', end - start)\n",
    "# # wandb.log({\"training_time\": end - start})\n",
    "\n",
    "# with plt.xkcd():\n",
    "#     plt.plot(train_loss_results)\n",
    "#     plt.title(\"Supervised Contrastive Loss\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_model():\n",
    "    input_ids = tf.keras.Input(shape=(MAX_SEQ_LEN,), batch_size=BATCH_SIZE, name=\"input_ids\", dtype=tf.int32)\n",
    "    attention_mask = tf.keras.Input(shape=(MAX_SEQ_LEN,), batch_size=BATCH_SIZE, name=\"attention_mask\", dtype=tf.int32)\n",
    "    token_type_ids = tf.keras.Input(shape=(MAX_SEQ_LEN,), batch_size=BATCH_SIZE, name=\"token_type_ids\", dtype=tf.int32)\n",
    "\n",
    "    bert_input = [input_ids, attention_mask, token_type_ids]\n",
    "#     encoder_r.trainable = False\n",
    "#     r = encoder_r(bert_input, training=False)\n",
    "\n",
    "    encoder_r.trainable = True\n",
    "    r = encoder_r(bert_input, training=True)\n",
    "    r = r[0][:,0,:]\n",
    "#     r = tf.keras.layers.GlobalAveragePooling1D()(r[0])\n",
    "    r = normalization_layer(r)\n",
    "    outputs = Dense(Num_class)(r)\n",
    "\n",
    "    supervised_model = tf.keras.Model(bert_input, outputs)\n",
    "\n",
    "    return supervised_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(32, 128)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mask (InputLayer)     [(32, 128)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids (InputLayer)     [(32, 128)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Model)                   ((32, 128, 768), (32 11683584    input_ids[0][0]                  \n",
      "                                                                 attention_mask[0][0]             \n",
      "                                                                 token_type_ids[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(32, 768)]          0           model[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "unit_norm_layer (UnitNormLayer) (32, 768)            0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (32, 6)              4614        unit_norm_layer[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 11,688,198\n",
      "Trainable params: 11,688,198\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "supervised_classifier = supervised_model()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "# metric = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "supervised_classifier.compile(optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=metric)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2,\n",
    "    restore_best_weights=True, verbose=2)\n",
    "\n",
    "supervised_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/albert/pooler/kernel:0', 'tf_albert_model/albert/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/albert/pooler/kernel:0', 'tf_albert_model/albert/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/albert/pooler/kernel:0', 'tf_albert_model/albert/pooler/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/albert/pooler/kernel:0', 'tf_albert_model/albert/pooler/bias:0'] when minimizing the loss.\n",
      "624/624 [==============================] - 3358s 5s/step - loss: 0.8747 - accuracy: 0.7856 - val_loss: 0.6858 - val_accuracy: 0.8246\n",
      "Epoch 2/2\n",
      "624/624 [==============================] - 3340s 5s/step - loss: 0.5700 - accuracy: 0.8621 - val_loss: 0.5696 - val_accuracy: 0.8367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fce044d5470>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervised_classifier.fit(train_dataset, steps_per_epoch= NUM_TRAIN_ITERATION,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=2,\n",
    "    callbacks= es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 58s 2s/step - loss: 0.6320 - accuracy: 0.8034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6319806575775146, 0.8034273982048035]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervised_classifier.evaluate(test_dataset, steps = NUM_TEST_ITERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
